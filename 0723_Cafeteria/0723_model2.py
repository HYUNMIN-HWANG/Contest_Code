import pandas as pd
import numpy as np
import xgboost

#1. DATA
train = pd.read_csv('../data/DACON/cafeteria_prediction/train.csv')
test = pd.read_csv('../data/DACON/cafeteria_prediction/test.csv')
submission = pd.read_csv('../data/DACON/cafeteria_prediction/sample_submission.csv')

print(train.shape)  # (1205, 12)
print(test.shape)   # (50, 10)
print(submission.shape) # (50, 3)


# ë©”ë‰´ ì´ë¦„ ë¹¼ê¸°
drops = ['ì¡°ì‹ë©”ë‰´', 'ì¤‘ì‹ë©”ë‰´', 'ì„ì‹ë©”ë‰´']

train = train.drop(drops, axis=1)
test = test.drop(drops, axis=1)

# ìš”ì¼ -> ìˆ«ì
train['ì›”'] = pd.DatetimeIndex(train['ì¼ì']).month
test['ì›”'] = pd.DatetimeIndex(test['ì¼ì']).month

train['ì¼'] = pd.DatetimeIndex(train['ì¼ì']).day
test['ì¼'] = pd.DatetimeIndex(test['ì¼ì']).day

weekday = {
    'ì›”': 1,
    'í™”': 2,
    'ìˆ˜': 3,
    'ëª©': 4,
    'ê¸ˆ': 5
}

train['ìš”ì¼'] = train['ìš”ì¼'].map(weekday)
test['ìš”ì¼'] = test['ìš”ì¼'].map(weekday)

# ë³¸ì‚¬ì •ì›ìˆ˜ - íœ´ê°€ì - ì¬íƒê·¼ë¬´ì
train['ì‹ì‚¬ê°€ëŠ¥ììˆ˜'] = train['ë³¸ì‚¬ì •ì›ìˆ˜'] - train['ë³¸ì‚¬íœ´ê°€ììˆ˜'] - train['í˜„ë³¸ì‚¬ì†Œì†ì¬íƒê·¼ë¬´ììˆ˜']
test['ì‹ì‚¬ê°€ëŠ¥ììˆ˜'] = test['ë³¸ì‚¬ì •ì›ìˆ˜'] - test['ë³¸ì‚¬íœ´ê°€ììˆ˜'] - test['í˜„ë³¸ì‚¬ì†Œì†ì¬íƒê·¼ë¬´ììˆ˜']

train['ì¤‘ì‹ì°¸ì—¬ìœ¨'] = train['ì¤‘ì‹ê³„'] / train['ì‹ì‚¬ê°€ëŠ¥ììˆ˜']
train['ì„ì‹ì°¸ì—¬ìœ¨'] = train['ì„ì‹ê³„'] / train['ì‹ì‚¬ê°€ëŠ¥ììˆ˜']

features = ['ì›”', 'ì¼', 'ìš”ì¼', 'ì‹ì‚¬ê°€ëŠ¥ììˆ˜', 'ë³¸ì‚¬ì¶œì¥ììˆ˜', 'ë³¸ì‚¬ì‹œê°„ì™¸ê·¼ë¬´ëª…ë ¹ì„œìŠ¹ì¸ê±´ìˆ˜']
labels = ['ì¤‘ì‹ê³„',	'ì„ì‹ê³„', 'ì¤‘ì‹ì°¸ì—¬ìœ¨', 'ì„ì‹ì°¸ì—¬ìœ¨']

train = train[features+labels]
test = test[features]

# print(train[:20])
# print(test.head())

# Modeling
from sklearn.ensemble import RandomForestRegressor
from catboost import CatBoostRegressor
from lightgbm import LGBMRegressor
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from sklearn.model_selection import GridSearchCV
from mlxtend.regressor import StackingCVRegressor
from sklearn.linear_model import Lasso

RANDOM_SEED = 2021
PROBAS = True
FOLDS = 8
N_ESTIMATORS = 1000
lasso = Lasso()

# ëŒ€íšŒ ê·œì¹™
# í‰ê°€ì‚°ì‹ : MAE(Mean Absolute Error)
lunch_model = RandomForestRegressor(criterion='mae')
dinner_model = RandomForestRegressor(criterion='mae')


XGB_params = {
    'learning_rate': [0.0, 0.1, 0.09, 0.089, 0.08]
}

lgb_params = {
    'metric': 'l1',
    'n_estimators': N_ESTIMATORS,
    'objective': 'regression',
    'random_state': RANDOM_SEED,
    'learning_rate': 0.01,
    'min_child_samples': 150,
    'reg_alpha': 3e-5,
    'reg_lambda': 9e-2,
    'num_leaves': 20,
    'max_depth': 16,
    'colsample_bytree': 0.8,
    'subsample': 0.8,
    'subsample_freq': 2,
    'max_bin': 240,
}

ctb_params = {
    'bootstrap_type': 'Poisson',
    'loss_function': 'MAE',
    'eval_metric': 'MAE',
    'random_seed': RANDOM_SEED,
    'task_type': 'GPU',
    'max_depth': 8,
    'learning_rate': 0.01,
    'n_estimators': N_ESTIMATORS,
    'max_bin': 280,
    'min_data_in_leaf': 64,
    'l2_leaf_reg': 0.01,
    'subsample': 0.8
}

rf_params = {
    'max_depth': 15,
    'min_samples_leaf': 8,
    'random_state': RANDOM_SEED
}


# ê²°ê³¼ë¥¼ ì„ì‹œë¡œ ì €ì¥í•  í”„ë ˆì„
temp_result = pd.DataFrame(index=['ì¼ì','ì¤‘ì‹ê³„','ì„ì‹ê³„','ìš”ì¼'])
print(temp_result.shape)
print(temp_result)


lunch_r = XGBRegressor(objective='reg:squarederror')
dinner_r = XGBRegressor(objective='reg:squarederror')

lunch_model = GridSearchCV(lunch_r, XGB_params, scoring='neg_mean_absolute_error')
dinner_model = GridSearchCV(dinner_r, XGB_params, scoring='neg_mean_absolute_error')

lunch_cat = CatBoostRegressor(n_estimators=1000)
dinner_cat = CatBoostRegressor(n_estimators=1000)

lunch_LGBM = LGBMRegressor(n_estimators=1000)
dinner_LGBM = LGBMRegressor(n_estimators=1000)

lunch_KN = KNeighborsRegressor(n_neighbors = 1)
dinner_KN = KNeighborsRegressor(n_neighbors = 1)

lunch_stack = StackingCVRegressor(regressors=(lunch_model, lunch_cat, lunch_LGBM, lunch_KN),
                            meta_regressor=lasso,
                            random_state=RANDOM_SEED)

dinner_stack = StackingCVRegressor(regressors=(dinner_model, dinner_cat, dinner_LGBM, dinner_KN),
                            meta_regressor=lasso,
                            random_state=RANDOM_SEED)                            

# ì¤‘ì‹
x = train[['ì›”', 'ì¼', 'ìš”ì¼', 'ì‹ì‚¬ê°€ëŠ¥ììˆ˜', 'ë³¸ì‚¬ì¶œì¥ììˆ˜', 'ë³¸ì‚¬ì‹œê°„ì™¸ê·¼ë¬´ëª…ë ¹ì„œìŠ¹ì¸ê±´ìˆ˜']]
y = train['ì¤‘ì‹ê³„']

lunch_stack.fit(x, y)

test_x = test[['ì›”', 'ì¼', 'ìš”ì¼', 'ì‹ì‚¬ê°€ëŠ¥ììˆ˜', 'ë³¸ì‚¬ì¶œì¥ììˆ˜', 'ë³¸ì‚¬ì‹œê°„ì™¸ê·¼ë¬´ëª…ë ¹ì„œìŠ¹ì¸ê±´ìˆ˜']]
y_pred = lunch_stack.predict(test_x)
submission['ì¤‘ì‹ê³„'] = y_pred

print("====================================================")

# ì„ì‹
x = train[['ì›”', 'ì¼', 'ìš”ì¼(ì„ì‹)', 'ì‹ì‚¬ê°€ëŠ¥ììˆ˜', 'ë³¸ì‚¬ì¶œì¥ììˆ˜', 'ë³¸ì‚¬ì‹œê°„ì™¸ê·¼ë¬´ëª…ë ¹ì„œìŠ¹ì¸ê±´ìˆ˜']]
y = train['ì„ì‹ê³„']

dinner_stack.fit(x, y)

test_x = test[['ì›”', 'ì¼', 'ìš”ì¼(ì„ì‹)', 'ì‹ì‚¬ê°€ëŠ¥ììˆ˜', 'ë³¸ì‚¬ì¶œì¥ììˆ˜', 'ë³¸ì‚¬ì‹œê°„ì™¸ê·¼ë¬´ëª…ë ¹ì„œìŠ¹ì¸ê±´ìˆ˜']]
y_pred = dinner_stack.predict(test_x)
submission['ì„ì‹ê³„'] = y_pred

print("=========================================")

submission.to_csv('../data/DACON/cafeteria_prediction/sub/0723_submit2.csv', index=False)
print(submission.shape)   # (50, 3)
print(submission.head())


print(" ğŸ’¯ğŸ’¯ğŸ’¯ğŸ’¯ Done!!! ") 

import pandas as pd
import numpy as np
import xgboost

#1. DATA
train = pd.read_csv('../data/DACON/cafeteria_prediction/train.csv')
test = pd.read_csv('../data/DACON/cafeteria_prediction/test.csv')
submission = pd.read_csv('../data/DACON/cafeteria_prediction/sample_submission.csv')

print(train.shape)  # (1205, 12)
print(test.shape)   # (50, 10)
print(submission.shape) # (50, 3)


# ë©”ë‰´ ì´ë¦„ ë¹¼ê¸°
drops = ['ì¡°ì‹ë©”ë‰´', 'ì¤‘ì‹ë©”ë‰´', 'ì„ì‹ë©”ë‰´']

train = train.drop(drops, axis=1)
test = test.drop(drops, axis=1)

# ìš”ì¼ -> ìˆ«ì
train['ì›”'] = pd.DatetimeIndex(train['ì¼ì']).month
test['ì›”'] = pd.DatetimeIndex(test['ì¼ì']).month

train['ì¼'] = pd.DatetimeIndex(train['ì¼ì']).day
test['ì¼'] = pd.DatetimeIndex(test['ì¼ì']).day

weekday = {
    'ì›”': 1,
    'í™”': 2,
    'ìˆ˜': 3,
    'ëª©': 4,
    'ê¸ˆ': 5
}

train['ìš”ì¼'] = train['ìš”ì¼'].map(weekday)
test['ìš”ì¼'] = test['ìš”ì¼'].map(weekday)

# ë³¸ì‚¬ì •ì›ìˆ˜ - íœ´ê°€ì - ì¬íƒê·¼ë¬´ì
train['ì‹ì‚¬ê°€ëŠ¥ììˆ˜'] = train['ë³¸ì‚¬ì •ì›ìˆ˜'] - train['ë³¸ì‚¬íœ´ê°€ììˆ˜'] - train['í˜„ë³¸ì‚¬ì†Œì†ì¬íƒê·¼ë¬´ììˆ˜']
test['ì‹ì‚¬ê°€ëŠ¥ììˆ˜'] = test['ë³¸ì‚¬ì •ì›ìˆ˜'] - test['ë³¸ì‚¬íœ´ê°€ììˆ˜'] - test['í˜„ë³¸ì‚¬ì†Œì†ì¬íƒê·¼ë¬´ììˆ˜']

train['ì¤‘ì‹ì°¸ì—¬ìœ¨'] = train['ì¤‘ì‹ê³„'] / train['ì‹ì‚¬ê°€ëŠ¥ììˆ˜']
train['ì„ì‹ì°¸ì—¬ìœ¨'] = train['ì„ì‹ê³„'] / train['ì‹ì‚¬ê°€ëŠ¥ììˆ˜']

features = ['ì›”', 'ì¼', 'ìš”ì¼', 'ì‹ì‚¬ê°€ëŠ¥ììˆ˜', 'ë³¸ì‚¬ì¶œì¥ììˆ˜', 'ë³¸ì‚¬ì‹œê°„ì™¸ê·¼ë¬´ëª…ë ¹ì„œìŠ¹ì¸ê±´ìˆ˜']
labels = ['ì¤‘ì‹ê³„',	'ì„ì‹ê³„', 'ì¤‘ì‹ì°¸ì—¬ìœ¨', 'ì„ì‹ì°¸ì—¬ìœ¨']

train = train[features+labels]
test = test[features]

# print(train[:20])
# print(test.head())

# Modeling
from sklearn.ensemble import RandomForestRegressor
from catboost import CatBoostRegressor
from lightgbm import LGBMRegressor
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from sklearn.model_selection import GridSearchCV
from mlxtend.regressor import StackingCVRegressor
from sklearn.linear_model import Lasso

RANDOM_SEED = 2021
PROBAS = True
FOLDS = 8
N_ESTIMATORS = 1000
lasso = Lasso()

# ëŒ€íšŒ ê·œì¹™
# í‰ê°€ì‚°ì‹ : MAE(Mean Absolute Error)
lunch_model = RandomForestRegressor(criterion='mae')
dinner_model = RandomForestRegressor(criterion='mae')


XGB_params = {
    'learning_rate': [0.0, 0.1, 0.09, 0.089, 0.08]
}

lgb_params = {
    'metric': 'l1',
    'n_estimators': N_ESTIMATORS,
    'objective': 'regression',
    'random_state': RANDOM_SEED,
    'learning_rate': 0.01,
    'min_child_samples': 150,
    'reg_alpha': 3e-5,
    'reg_lambda': 9e-2,
    'num_leaves': 20,
    'max_depth': 16,
    'colsample_bytree': 0.8,
    'subsample': 0.8,
    'subsample_freq': 2,
    'max_bin': 240,
}

ctb_params = {
    'bootstrap_type': 'Poisson',
    'loss_function': 'MAE',
    'eval_metric': 'MAE',
    'random_seed': RANDOM_SEED,
    'task_type': 'GPU',
    'max_depth': 8,
    'learning_rate': 0.01,
    'n_estimators': N_ESTIMATORS,
    'max_bin': 280,
    'min_data_in_leaf': 64,
    'l2_leaf_reg': 0.01,
    'subsample': 0.8
}

rf_params = {
    'max_depth': 15,
    'min_samples_leaf': 8,
    'random_state': RANDOM_SEED
}

# ìš”ì¼ë³„ë¡œ ì˜¤ë¦„ì°¨ìˆœ
train = train.sort_values(by='ìš”ì¼', ascending=True)
test = test.sort_values(by='ìš”ì¼', ascending=True)
submission['ìš”ì¼'] = test['ìš”ì¼']   # submission ì—ë„ 'ìš”ì¼' ì»¬ëŸ¼ ì¶”ê°€
submission = submission.sort_values(by='ìš”ì¼', ascending=True)
# print(test[:10])
# print(submission[:10])

# ê²°ê³¼ë¥¼ ì„ì‹œë¡œ ì €ì¥í•  í”„ë ˆì„
temp_result = pd.DataFrame(index=['ì¼ì','ì¤‘ì‹ê³„','ì„ì‹ê³„','ìš”ì¼'])
print(temp_result.shape)
print(temp_result)


# ìš”ì¼ë³„ë¡œ ë¶„ì„í•˜ê¸°
for day in range(1,6):

    lunch_r = XGBRegressor(objective='reg:squarederror')
    dinner_r = XGBRegressor(objective='reg:squarederror')

    lunch_model = GridSearchCV(lunch_r, XGB_params, scoring='neg_mean_absolute_error')
    dinner_model = GridSearchCV(dinner_r, XGB_params, scoring='neg_mean_absolute_error')

    lunch_cat = CatBoostRegressor(**ctb_params)
    dinner_cat = CatBoostRegressor(**ctb_params)

    lunch_LGBM = LGBMRegressor(**lgb_params)
    dinner_LGBM = LGBMRegressor(**lgb_params)

    lunch_KN = KNeighborsRegressor(n_neighbors = 1)
    dinner_KN = KNeighborsRegressor(n_neighbors = 1)


    lunch_stack = StackingCVRegressor(regressors=(lunch_model, lunch_cat, lunch_LGBM, lunch_KN),
                                meta_regressor=lasso,
                                random_state=RANDOM_SEED)

    dinner_stack = StackingCVRegressor(regressors=(dinner_model, dinner_cat, dinner_LGBM, dinner_KN),
                                meta_regressor=lasso,
                                random_state=RANDOM_SEED)                            

    # ìš”ì¼ë³„ë¡œ ë¶„ì„í•˜ê¸°        
    print(day, "ìš”ì¼ ë°ì´í„°ë§Œ ì¶”ì¶œ==================================")

    train_day = train[train['ìš”ì¼'] == day]
    print("train_day", train_day.shape)

    # ì˜ˆì¸¡ ë°ì´í„°
    test_day = test[test['ìš”ì¼'] == day]
    print("test_day", test_day.shape)
    predict_day = submission[submission['ìš”ì¼'] == day]
    print("predict_day", predict_day.shape)

    # ì¤‘ì‹
    x = train_day[['ì›”', 'ì¼', 'ìš”ì¼', 'ì‹ì‚¬ê°€ëŠ¥ììˆ˜', 'ë³¸ì‚¬ì¶œì¥ììˆ˜', 'ë³¸ì‚¬ì‹œê°„ì™¸ê·¼ë¬´ëª…ë ¹ì„œìŠ¹ì¸ê±´ìˆ˜']]
    y = train_day['ì¤‘ì‹ê³„']
    print(x.shape, y.shape)

    lunch_stack.fit(x, y)
    # print(lunch_stack.best_score_, lunch_stack.best_params_)
    # lunch_stack = lunch_stack.best_estimator_

    test_x = test_day[['ì›”', 'ì¼', 'ìš”ì¼', 'ì‹ì‚¬ê°€ëŠ¥ììˆ˜', 'ë³¸ì‚¬ì¶œì¥ììˆ˜', 'ë³¸ì‚¬ì‹œê°„ì™¸ê·¼ë¬´ëª…ë ¹ì„œìŠ¹ì¸ê±´ìˆ˜']]
    y_pred = lunch_stack.predict(test_x)
    predict_day['ì¤‘ì‹ê³„'] = y_pred
    print(predict_day)

    # ì¤‘ì‹
    x = train_day[['ì›”', 'ì¼', 'ìš”ì¼', 'ì‹ì‚¬ê°€ëŠ¥ììˆ˜', 'ë³¸ì‚¬ì¶œì¥ììˆ˜', 'ë³¸ì‚¬ì‹œê°„ì™¸ê·¼ë¬´ëª…ë ¹ì„œìŠ¹ì¸ê±´ìˆ˜']]
    y = train_day['ì„ì‹ê³„']
    print(x.shape, y.shape)

    dinner_stack.fit(x, y)
    # print(dinner_stack.best_score_, dinner_stack.best_params_)
    # dinner_stack = dinner_stack.best_estimator_

    test_x = test_day[['ì›”', 'ì¼', 'ìš”ì¼', 'ì‹ì‚¬ê°€ëŠ¥ììˆ˜', 'ë³¸ì‚¬ì¶œì¥ììˆ˜', 'ë³¸ì‚¬ì‹œê°„ì™¸ê·¼ë¬´ëª…ë ¹ì„œìŠ¹ì¸ê±´ìˆ˜']]
    y_pred = dinner_stack.predict(test_x)
    predict_day['ì„ì‹ê³„'] = y_pred
    print(predict_day)

    temp_result = pd.concat([temp_result, predict_day])

    # print(temp_result)

final_submission = temp_result.drop('ìš”ì¼', axis=1)
final_submission = final_submission.sort_values(by='ì¼ì', ascending=True)
final_submission = final_submission[:-4]
print(final_submission)


print("=========================================")

final_submission.to_csv('../data/DACON/cafeteria_prediction/sub/0723_submit1.csv', index=False)
print(final_submission.shape)   # (50, 3)
print(final_submission.head())



print(" ğŸ’¯ğŸ’¯ğŸ’¯ğŸ’¯ Done!!! ") 

# 0723_submit1.csv
# 80.7592468887	